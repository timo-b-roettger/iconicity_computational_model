---
title: "computational model for reference game"
format: html
editor: visual
---

# Communication Game Simulation

## Set up

```{r}

# libraries
library(tidyverse)
library(ggplot2)

# seed
# set.seed(123)

```

## Parameters

```{r}

# general parameters
n_sim <- 10 # number of simulations
n_referents <- 6 # number of unique referents in guessing game
n_small <- n_referents/2 # number of small referents
n_large <- n_referents/2 # number of large referents
n_rounds <- 1000 # number of interaction rounds
drift_sd <- 0.05 # amount of variation introduced during production
learn_rate <- 0.005 # amount of added memory strengthening for words per round
iconicity_boost <- 0.2 # multiplicator for distance to iconic prototypes

# referent properties
referents <- tibble(
  id = 1:n_referents,
  type = c(rep("small", n_small), rep("large", n_large))
) 

```

## Functions

```{r}

# distance-based ease function
## small referents easier to guess near [0,0]
## large referents easier to guess near [1,1]
## currently distance measures range from 0.014 (max distance), to 1 (min distance) and is sharply dropping off when away from targets
### NOTE: Discuss and justify distance function (maybe euclidean)
signal_ease <- function(sig, type) {
  target <- if (type == "small") c(0, 0) else c(1, 1)
  dist <- sqrt((sig[1] - target[1])^2 + (sig[2] - target[2])^2)
  ease <- exp(-1 * dist)   # the -2 value controls the sharpness of the drop with distance
  ease
}

# show heatmap of ease relative to signal space
grid <- expand.grid(
  x = seq(0, 1, length.out = 50),
  y = seq(0, 1, length.out = 50),
  type = c("small", "large")
)

# make heat map
grid <- grid %>%
  rowwise() %>%
  mutate(ease = signal_ease(c(x, y), type)) %>%
  ungroup()

ggplot(grid,
       aes(x = x, y = y, fill = ease)) +
  geom_tile() +
  facet_wrap(. ~type) +
  scale_fill_continuous(palette = "viridis")


# interpretation probability
## final guess probability = previous guessing rate + signal fit (iconicity bias)
guess_probability <- function(agent_guess, signal_xy, ref_type) {
  ease <- signal_ease(signal_xy, ref_type)

  # learned ability plus signal ease, capped at 1
  # the added learning boost of ease is reduced by a factor 'iconicity_boost' which represents the strength of iconicity affecting guessing
  p <- agent_guess + iconicity_boost * ease
  p[p > 0.95] <- 0.95 ## NOTE: really sloppy and should probably use a function that operates with logits or something, but this keeps a lapse rate of 5% atm
  p
}

# drift function
## adds gaussian drift to the signal 
apply_drift <- function(sig) {
  # pmax and pmin cap it to 0 and 1
  pmax(pmin(sig + rnorm(2, 0, drift_sd), 1),0)
}

```

## Main interaction loop

```{r}

# set up df to populate
history <- data.frame(
  round     = integer(),
  trial     = integer(),
  referent  = integer(),
  speaker   = character(),
  listener  = character(),
  type      = character(),
  p_correct = numeric(),
  success   = numeric(),
  signal_x  = numeric(),
  signal_y  = numeric(),
  stringsAsFactors = FALSE
)

# main for loop over interactions
for (n in 1:n_sim) {
  
# initial signals
signals <- tibble(
  id = referents$id,
  x = rep(0.5, n_referents),
  y = rep(0.5, n_referents)
)

# initial learning status of referents (after training)
## each agent has initial probability ~ 0.3 ± noise
agentA_guess <- rbeta(n_referents, 2, 4)
agentB_guess <- rbeta(n_referents, 2, 4)

## alternatives: average prob 0.5  = (2,2)
## alternatives: average prob 0.4  = (2,3)
## alternatives: average prob 0.33 = (2,4)

# main for loop within interaction
for (t in 1:n_rounds) {

  # speakers/listeners are taking turns ✅
  if (t %% 2 == 1) {
    speaker <- "A"
    listener <- "B"
    speaker_guess <- agentA_guess
    listener_guess <- agentB_guess
  } else {
    speaker <- "B"
    listener <- "A"
    speaker_guess <- agentB_guess
    listener_guess <- agentA_guess
  }

  # randomly pick one referent ✅
  r <- sample(1:n_referents, 1)
  r_type <- referents$type[r]

  # retrieve its signal: 
  ## either just pick the signal from last round (perfect memmory of signal)
  #sig <- signals %>% filter(id == r) %>% select(x, y) %>% as.numeric()
  ## alternatively: retrieve it's previous signal if memory strength good enough
  if (speaker_guess[r] > 0.5) {
    sig <- signals %>% filter(id == r) %>% select(x, y) %>% as.numeric()
    # if not randomly draw around center
    } else {sig <- c(rnorm(1, 0.5, 0.1), c(rnorm(1, 0.5, 0.1)))
    ## another alternative could be to draw from existing categories, which would make the signal bounce around
  }
  # this resets the signal drift until learned 
  
  # listener computes probability of correct guess
  p_correct <- guess_probability(listener_guess[r], sig, r_type)

  # actual outcome, binomial sampling
  success <- rbinom(1, 1, p_correct)

  # learning: speaker improves guess rate
  speaker_guess[r] <- speaker_guess[r] + learn_rate
  #if (speaker_guess[r] > 0.5) speaker_guess[r] <- 0.5  # cap learned component

  # listener also learns due to feedback
  if (success == 1) {
    listener_guess[r] <- listener_guess[r] + learn_rate
    #if (listener_guess[r] > 0.5) listener_guess[r] <- 0.5
  }
  ## NOTE: have to think of the relationship between learning rate, producing the pair vs. hearing the pair, and successful vs. unsuccessful guessing. 

  # if the guess was successful, the signal does not need to change
  # if the guess was unsuccessful, the signal drifts
  if (success == 0) {
    #listener_guess[r] <- listener_guess[r] + learn_rate
    new_sig <- apply_drift(sig)
    signals$x[r] <- new_sig[1]
    signals$y[r] <- new_sig[2]
  }

  # update guessing probability based on learning
  if (speaker == "A") {
    agentA_guess <- speaker_guess
    agentB_guess <- listener_guess
  } else {
    agentB_guess <- speaker_guess
    agentA_guess <- listener_guess
  }

  # log trials
  history <- rbind(
      history,
      data.frame(
        sim      = n,
        round    = t,
        referent = r,
        speaker  = speaker,
        listener = listener,
        type     = r_type,
        p_correct = p_correct,
        success  = success,
        signal_x = signals$x[r],
        signal_y = signals$y[r]
      )
  )
  
} # end of n_rounds loop

} # end of n_sim loop  
  
```

## Plot results

```{r}

# Aggregate
accuracy_all <- history |> 
  group_by(round) |> 
  summarise(success = mean(success))

accuracy <- history |> 
  group_by(sim, round) |> 
  summarise(success = mean(success))

# Success rate over time
ggplot(accuracy, aes(round, success)) +
  geom_smooth(method = "glm", 
              method.args = list(family = "binomial"), 
              se = TRUE) +
  labs(title = "Success Rate Over Time", 
       y = "Success", 
       x = "Round") +
  facet_wrap(. ~ sim) +
  scale_y_continuous(limits = c(0,1)) + 
  theme_minimal()

```

```{r}

# Signal space use
ggplot(history, aes(signal_x, signal_y, color = type)) +
  geom_point(alpha = 0.2) +
  labs(title = "Evolution of Signal Space", x = "x", y = "y") +
  facet_wrap(. ~ sim) +
  scale_x_continuous(limits = c(0,1)) + 
  scale_y_continuous(limits = c(0,1)) + 
  scale_fill_continuous(palette = "viridis") +
  theme_minimal()

```

```{r}

# Calculate iconicity across bins
hist_agg <- history |> 
  mutate(bins = cut(round, breaks = 20, labels = FALSE)) |> 
  group_by(bins, type, sim) |> 
  summarise(signal_x = mean(signal_x),
            signal_y = mean(signal_y)) 

iconicity <- hist_agg |> 
  mutate(target_x = ifelse(type == "small", 0, 1),
         target_y = ifelse(type == "small", 0, 1),
         dist = sqrt((signal_x - target_x)^2 + (signal_y - target_y)^2),
         iconicity = exp(-2*dist)) |> 
  group_by(bins, sim) |> 
  summarise(iconicity = mean(iconicity)) 

iconicity_agg <- iconicity |> 
  group_by(bins) |> 
  summarise(iconicity = mean(iconicity)) 

# Iconicity over time
ggplot(iconicity, aes(x = bins, 
                      y = iconicity,
                      group = sim)) +
  geom_path(size = 1, alpha = 0.2) +
  geom_path(data = iconicity_agg,
            aes(group = 1),
            size = 2) +
  # starting point with signal = [0.5,0.5]
  geom_hline(yintercept = 0.2431167, 
               lty = "dashed") +
  scale_y_continuous(limits = c(0,1),
                     breaks = c(0,0.25,0.5,0.75,1)) + 
  labs(title = "Iconicity Over Time", 
       y = "Iconicity (high = iconic, low = not iconic)", 
       x = "Rounds in 100s") +
  #scale_y_continuous(limits = c(0,1)) + 
  theme_minimal()

```
